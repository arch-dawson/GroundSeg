\documentclass[10pt,letterpaper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\R}{\mathbb{R}}
\author{Dawson Beatty}
\title{Matrix Methods Homework 9 \\ APPM 3310-003}
\begin{document}
	\maketitle
	
	\section*{Section 5.3}
	\subsection*{27}
	\subsubsection*{(a)} \textbf{Find the $QR$ factorization of the following: $A = \begin{bmatrix}
		1 & -3 \\ 2 & 1
		\end{bmatrix} = \begin{bmatrix}
		v_1 & v_2
		\end{bmatrix}$}
		
		\begin{align*}
		& u_1 = \frac{v_1 - ||v_1||e_1}{||v_1 - ||v_1||e_1||} = \frac{\begin{bmatrix}
			1 \\ 2
			\end{bmatrix} - \sqrt{5} \begin{bmatrix}
			1 \\ 0
			\end{bmatrix}}{2.35} = \begin{bmatrix}
		\frac{20 - 20 \sqrt{5}}{47} \\ \frac{40}{47}
		\end{bmatrix} \\ 
		& H_1 = I - 2u_1u_1^T = I - 2 \begin{bmatrix}
		\frac{20 - 20 \sqrt{5}}{47} \\ \frac{40}{47}
		\end{bmatrix} \begin{bmatrix}
		\frac{20 - 20 \sqrt{5}}{47} & \frac{40}{47}
		\end{bmatrix} = I- \begin{bmatrix}
		.54 & -.88 \\ -.88 & 1.44
		\end{bmatrix} \\ 
		& H_1 = \boxed{\begin{bmatrix}
		.46 & -.88 \\ -.88 & -.44
		\end{bmatrix} = Q} \\ 
		& \boxed{R = HA = \begin{bmatrix}
		-2.24 & .447 \\ 0  & 3.13
		\end{bmatrix}}
		\end{align*}
	\subsection*{28}
	\subsubsection*{(a)} \textbf{Find the $QR$ factorization of the following coefficient matrix}
	\begin{align*}
	& \begin{bmatrix}
	1 & 2 \\ -1 & 3
	\end{bmatrix} \begin{bmatrix}
	x \\ y
	\end{bmatrix} = \begin{bmatrix}
	-1 \\ 2
	\end{bmatrix} \\ 
	& A = \begin{bmatrix}
	1 & 2 \\ -1 & 3
	\end{bmatrix} = \begin{bmatrix}
	v_1 & v_2
	\end{bmatrix} \\ 
	&  u_1 = \frac{v_1 - ||v_1||e_1}{||v_1 - ||v_1||e_1||} = \frac{\begin{bmatrix}
		1 \\ -1
		\end{bmatrix} - \sqrt{2} \begin{bmatrix}
		1 \\ 0
		\end{bmatrix}}{1.0824} = \begin{bmatrix}
	\frac{1 - \sqrt{2}}{1.0824} \\ \frac{-1}{1.0824}
	\end{bmatrix} \\ 
	& H_1 = I -2u_1u_1^T = I - 2 \begin{bmatrix}
	\frac{1 - \sqrt{2}}{1.0824} \\ \frac{-1}{1.0824}
	\end{bmatrix} \begin{bmatrix}
	\frac{1 - \sqrt{2}}{1.0824} & \frac{-1}{1.0824}
	\end{bmatrix} = I - \begin{bmatrix}
	.2929 & .7071 \\ .7071 & 1.7071 
	\end{bmatrix} \\ 
	& H_1 = \boxed{\begin{bmatrix}
	.7071 & -.7071 \\ -.7071 & .7071 
	\end{bmatrix} = Q} \\ 
	& \boxed{R = H_1A = \begin{bmatrix}
		-1.414 & .7071  \\ 0 & 3.535
		\end{bmatrix} }
	\end{align*}
	\subsubsection*{(b)} \textbf{Solve the above system using $Rx = Q^Tb$}
	\begin{align*}
	& \begin{bmatrix}
	-1.414 & .7071  \\ 0 & 3.535
	\end{bmatrix} \begin{bmatrix}
	x \\ y
	\end{bmatrix} = \begin{bmatrix}
	.7071 & -.7071 \\ -.7071 & .7071 
	\end{bmatrix} \begin{bmatrix}
	-1 \\ 2
	\end{bmatrix} \\ 
	& -1.414x + .7071y = 2.1213 \\ 
	& 3.535y = .7071 \\ 
	& \boxed{\begin{bmatrix}
		x \\ y
		\end{bmatrix} = \begin{bmatrix}
		-1.4 \\ .2
		\end{bmatrix}}
	\end{align*}
	\section*{Section 5.5}
	\subsection*{2} \textbf{Find the projection of the vector $v = \begin{bmatrix}
		1 \\ 1 \\ 1
		\end{bmatrix}$ into the following}
		\subsubsection*{(a)} \textbf{The vector $v_1 = \begin{bmatrix}
			\frac{-1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}}
			\end{bmatrix}$} 
		\begin{align*}
		w = \frac{v \cdot v_1}{||v_1||^2}v_1 = \frac{1}{\sqrt{3}} \begin{bmatrix}
		\frac{-1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}}
		\end{bmatrix} = \boxed{ \begin{bmatrix}
		\frac{-1}{3} \\ \frac{1}{3} \\ \frac{1}{3}
 		\end{bmatrix} }
		\end{align*}
		\subsubsection*{(b)}  \textbf{The vector $v_1 = \begin{bmatrix}
			2 \\ -1 \\ 3
			\end{bmatrix}$} 
		\begin{align*}
		w = \frac{v \cdot v_1}{||v_1||^2}v_1 = \frac{4}{14} \begin{bmatrix}
		2 \\ -1 \\ 3
		\end{bmatrix} = \boxed{ \begin{bmatrix}
			\frac{4}{7} \\ \frac{-2}{7} \\ \frac{6}{7}
			\end{bmatrix} }
		\end{align*}
		\subsubsection*{(c)} \textbf{Plane spanned by $v_1 = \begin{bmatrix}
			1 \\ 1 \\ 0
			\end{bmatrix}, v_2 = \begin{bmatrix}
			-2 \\ 2 \\ 1
			\end{bmatrix}$} 
			$$
			w = \frac{v \cdot v_1}{||v_1||^2} v_1 + \frac{v \cdot v_2}{||v_2||^2} v_2 = \frac{2}{2} \begin{bmatrix}
			1 \\ 1 \\ 0
			\end{bmatrix} + \frac{1}{9} \begin{bmatrix}
			-2 \\ 2 \\ 1
			\end{bmatrix} = \boxed{ \begin{bmatrix}
			\frac{7}{9} \\ \frac{11}{9} \\ \frac{1}{9}
			\end{bmatrix} }
			$$
		\subsubsection*{(d)} \textbf{Plane spanned by $v_1 = \begin{bmatrix}
			-\frac{3}{5} \\ \frac{4}{5} \\ 0
			\end{bmatrix}, v_2 = \begin{bmatrix}
			\frac{4}{13} \\ \frac{3}{13} \\ -\frac{12}{13}
			\end{bmatrix}$} 
		$$
		w = \frac{v \cdot v_1}{||v_1||^2} v_1 + \frac{v \cdot v_2}{||v_2||^2} v_2 = \frac{\frac{1}{5}}{1} \begin{bmatrix}
		-\frac{3}{5} \\ \frac{4}{5} \\ 0
		\end{bmatrix} + \frac{\frac{-5}{13}}{1} \begin{bmatrix}
		\frac{4}{13} \\ \frac{3}{13} \\ -\frac{12}{13}
		\end{bmatrix} = \boxed{ \begin{bmatrix}
			\frac{-1007}{4225} \\ \frac{301}{4225} \\ \frac{12}{5}
			\end{bmatrix} }
		$$
	\subsection*{9} 
	\subsubsection*{(a)} 
	\begin{proof}
		\textbf{Seek to prove that the set of all vectors orthogonal to a given subspace $V \subset \R^m$ forms a subspace.}
		
		Have to check closed under addition, scalar multiplication, and contains 0. All other subspace axioms follow from the fact that this is a smaller subspace of a valid existing subspace. 
		
		\textit{Addition: } By definition, the set of all vectors orthogonal to $V$ are in the range of at most $m-1$ basis vectors.  If any vectors (or the sum of any vectors in the set) were able to leave the range of these basis vectors, it would not have been an orthogonal set to begin with.  This is because we would need another basis vector to express something outside the existing range. 
		
		\textit{Scalar Multiplication: } As with above, all of the vectors in our orthogonal set lie in the range of at least $m-1$ basis vectors.  The range is composed of all possible linear combinations of these basis vectors, which already includes any possible scalar multiples of vectors in the subspace. 
		
		\textit{Contains $\vec{0}$: } The zero vector is orthogonal to all vectors, so it must be in the set of all vectors orthogonal to a given subspace.  
	\end{proof}
	
	\subsubsection*{(b)} \textbf{Find  basis for the set of all vectors in $\R^4$ that are orthogonal to the subspace spanned by $v_1 = \begin{bmatrix}
		1 \\ 2 \\ 0 \\ 1
		\end{bmatrix}, v_2 = \begin{bmatrix}
		2 \\ 0 \\ 3 \\ 1
		\end{bmatrix}$} 
		
		For a vector $\vec{w} = \begin{bmatrix}
		a \\ b \\ c \\ d 
		\end{bmatrix}$ to be orthogonal to the above vectors, both $w \cdot v_1$ and $w \cdot v_2$ must be zero.  In other words, 
		\begin{align*}
		& a + 2b + d = 0 & 2a + 3c + d = 0
		\end{align*}
		
		From this point it's trivial to find a basis, setting first $a$ then $b$ equal to zero, arriving at a basis of 
		$$
		\boxed{\begin{bmatrix}
			0 \\ 3 \\ 2 \\ -6
			\end{bmatrix}, \begin{bmatrix}
			-3 \\ 0 \\ 1 \\ 3
			\end{bmatrix}}
		$$
	\subsection*{10} \textbf{Find the closest point to the vector $b = \begin{bmatrix}
		1 \\ 0 \\ 3
		\end{bmatrix}$ belonging to the plane spanned by $v_1 = \begin{bmatrix}
		1 \\ -1 \\ 1
		\end{bmatrix}, v_2 = \begin{bmatrix}
		-1 \\ 1 \\ 2
		\end{bmatrix}$}
		
		Vectors are already orthogonal, so we can jump right into orthogonal projection without Gram-Schmidt
		$$
		w = \frac{b \cdot v_1}{||v_1||^2}v_1 + \frac{b \cdot v_2}{||v_2||^2}v_2 = \frac{3}{3} \begin{bmatrix}
		1 \\ -1 \\ 1
		\end{bmatrix} + \frac{3}{6} \begin{bmatrix}
		-1 \\ 1 \\  2
		\end{bmatrix} \boxed{= \begin{bmatrix}
			\frac{1}{2} \\ -\frac{1}{2} \\ 2
			\end{bmatrix}}
		$$
	\subsection*{11} \textbf{Find the least squares solution to the following linear systems. }
	\subsubsection*{(a)} 
	$$
	\begin{bmatrix}
	1 & -1 \\ 2 & 2 \\ 3 & -1
	\end{bmatrix} \begin{bmatrix}
	x \\ y
	\end{bmatrix} = \begin{bmatrix}
	1 \\ 0 \\ -1
	\end{bmatrix}
	$$
	The columns of the coefficient matrix are already orthogonal, so we can skip Gram-Schmidt. $b$ is not in the range of $A$, so we'll find the closest vector within $A$ to $b$, which we'll call $w$. We'll then find the least squares solution to that system. 
	$$
	w = \frac{b \cdot v_1 }{||v_1||^2}v_1 + \frac{b \cdot v_2}{||v_2||^2}v_2 = \frac{-2}{14} \begin{bmatrix}
	1 \\ 2 \\ 3
	\end{bmatrix} + \frac{0}{6} \begin{bmatrix}
	-1 \\ 2 \\ -1
	\end{bmatrix} = \begin{bmatrix}
	-\frac{1}{7} \\ -\frac{2}{7} \\ -\frac{3}{7}
	\end{bmatrix}
	$$
	Now that we have the closest vector in the range of the coefficient matrix, now we solve $Ax=w$
	
\begin{align*}
& \begin{bmatrix}
1 & -1 & | & -\frac{1}{7} \\ 
2 & 2 & | & - \frac{2}{7} \\
3 & -1 & | & -\frac{3}{7}
\end{bmatrix} \begin{array}{c}
\\ \\ -\frac{3}{2} R_2
\end{array} = \begin{bmatrix}
1 & -1 & | & -\frac{1}{7} \\ 
2 & 2 & | & - \frac{2}{7} \\
0 & 4 & | & 0
\end{bmatrix} \begin{array}{c}
\\ -2 R_1 \\ .
\end{array} \\ 
& \begin{bmatrix}
1 & -1 & | & -\frac{1}{7} \\ 
0 & 4 & | & 0 \\
0 & -4 & | & 0
\end{bmatrix} \begin{array}{c}
+ R_2
\end{array} = \begin{bmatrix}
1 & -1 & | & -\frac{1}{7} \\ 
0 & 4 & | & 0 \\
0 & 0 & | & 0
\end{bmatrix} \\ 
& \boxed{\begin{bmatrix}
x \\ y
\end{bmatrix} = \begin{bmatrix}
-\frac{1}{7} \\ 0
\end{bmatrix}}
\end{align*}
	\subsubsection*{(b)} 
	\subsection*{23} \textbf{Apply the method from problem \textit{5.5.22} to find the least squares solution to the systems in excercise \textit{4.3.14}. }
	\subsubsection*{(a)} 
	$$
	Ax = b \Rightarrow  \begin{bmatrix}
	w_1 & w_2
	\end{bmatrix} x = b \Rightarrow \begin{bmatrix}
	1 & 2 \\ 3 & -1 \\ -1 & 2
	\end{bmatrix} \begin{bmatrix}
	x \\ y
	\end{bmatrix} = \begin{bmatrix}
	1 \\ 0 \\ 3
	\end{bmatrix}
	$$ 
	
	First we apply Gram-Schmidt: 
	\begin{align*}
	& v_1 = w_1 & A = \begin{bmatrix}
	a_1 & a_2
	\end{bmatrix} \\ 
	& v_2 = w_2 - \frac{w_2 \cdot v_1}{||v_1||^2}v_1 = \begin{bmatrix}
	2 \\ -1 \\ 2
	\end{bmatrix} - \frac{-3}{11} \begin{bmatrix}
	1 \\ 3 \\ -1
	\end{bmatrix} = \begin{bmatrix}
	\frac{25}{11} \\ \frac{-2}{11} \\ \frac{19}{11}
	\end{bmatrix} \\ 
	& e_1 = \frac{v_1}{||v_1||} = \begin{bmatrix}
	\frac{379}{1257} \\ \frac{379}{419} \\ \frac{-379}{1257}
	\end{bmatrix} & e_2 = \frac{v_2}{||v_2||} = \begin{bmatrix}
	\frac{700}{881} \\ \frac{-56}{881} \\ \frac{532}{881}
	\end{bmatrix} \\ 
	& Q = \begin{bmatrix}
	e_1 & e_2
	\end{bmatrix} & R = \begin{bmatrix}
	a_1 \cdot e_1 & a_2 \cdot e_1 \\ 0 & a_2 \cdot e_2
	\end{bmatrix} \\ 
	& Q = \begin{bmatrix}
		\frac{379}{1257} & 	\frac{700}{881} \\ \frac{379}{419} & \frac{-56}{881} \\ \frac{-379}{1257} & \frac{532}{881}
		\end{bmatrix} \qquad R = \begin{bmatrix}
		\frac{1456}{439} & \frac{-379}{419} \\ 0 & \frac{2520}{881}
		\end{bmatrix}
	\end{align*} 
	Now solve using $Rx = Q^Tb$ 
	
	\begin{align*}
	& \begin{bmatrix}
	\frac{1456}{439} & \frac{-379}{419} \\ 0 & \frac{2520}{881}
	\end{bmatrix} \begin{bmatrix}
	x \\ y
	\end{bmatrix} = \begin{bmatrix}
	\frac{379}{1257} & \frac{379}{419} & \frac{-379}{1257} \\ 
	\frac{700}{881} & \frac{-56}{881} & \frac{532}{881}
	\end{bmatrix} \begin{bmatrix}
	1 \\ 0 \\ 3
	\end{bmatrix} \\ 
	& \frac{1456}{439}x - \frac{379}{419}y = -\frac{758}{1257} & \frac{2520}{881} = \frac{1191}{457} \\
	& \boxed{\begin{bmatrix}
		x\\y
		\end{bmatrix} = \begin{bmatrix}
		.067 \\ .911
		\end{bmatrix}}
	\end{align*}
	
	\subsubsection*{(b)}
	$$
	A = \begin{bmatrix}
	4 & -2 \\ 2 & 3 \\ 1 & -2 \\ 2 & 2
	\end{bmatrix} \begin{bmatrix}
	x \\ y
	\end{bmatrix} = \begin{bmatrix}
	1 \\ -2 \\ -1 \\ 2
	\end{bmatrix}
	$$
	\begin{align*}
	& e_1 = \frac{a_1}{||a_1||} = \begin{bmatrix}
	\frac{4}{5} \\ \frac{2}{5}  \\ \frac{1}{5} \\ \frac{2}{5}
	\end{bmatrix} & e_2 = \frac{a_2}{||a_2||} = \begin{bmatrix}
	\frac{-769}{1762} \\ \frac{2089}{3191}  \\ \frac{-769}{1762} \\ \frac{769}{1762}
	\end{bmatrix} \\
	& Q = \begin{bmatrix}
	e_1 & e_2 
	\end{bmatrix} = \begin{bmatrix}
	\frac{4}{5} & \frac{-769}{1762}\\ \frac{2}{5} & \frac{2089}{3191}  \\ \frac{1}{5} & \frac{-769}{1762} \\ \frac{2}{5} & \frac{769}{1762}
	\end{bmatrix} &  R = \begin{bmatrix}
	a_1 \cdot e_1 & a_1 \cdot e_2 \\ 0 & a_2 \cdot e_2
	\end{bmatrix} = \begin{bmatrix}
	5 & 0 \\ 0 & \frac{3524}{769}
	\end{bmatrix}
	\end{align*}
	Now we can solve $Rx = Q^Tb$.
	\begin{align*}
	& \begin{bmatrix}
	5 & 0 \\ 0 & \frac{3524}{769}
	\end{bmatrix} \begin{bmatrix}
	x \\ y
	\end{bmatrix} = \begin{bmatrix}
	\frac{4}{5} & \frac{2}{5}  & \frac{1}{5} & \frac{2}{5} \\ 
	\frac{-769}{1762} & \frac{2089}{3191}  & \frac{-769}{1762} & \frac{769}{1762}
	\end{bmatrix} \begin{bmatrix}
	1 \\ -4 \\ -1 \\ 2
	\end{bmatrix} \\ 
	& 5x = -\frac{1}{5} & \frac{3524}{769}y = -\frac{1538}{881} \\
	& \boxed{\begin{bmatrix}
		x \\ y
		\end{bmatrix} = \begin{bmatrix}
		1 \\ -.381
		\end{bmatrix}}
	\end{align*}
	\section*{Section 5.6}
	\subsection*{2} \textbf{Find a basis for the orthogonal complement to the following subspaces of $\R^3$} 
	\subsubsection*{(a)} \textbf{The plane $3x + 4y - 5z = 0$} 
	
	Need $\begin{bmatrix}
	3 \\ 4 \\ -5
	\end{bmatrix} \cdot \begin{bmatrix}
	a \\ b \\ c
	\end{bmatrix} = 0, \qquad 3a + 4b - 5c = 0$
	
	We'll arbitrarily use $\begin{bmatrix}
		3 \\ 4 \\ 5
	\end{bmatrix}$ because we like whole numbers. We can use this as the basis: 
	$$
	\boxed{\text{span} \left\lbrace  \begin{bmatrix}
			3 \\ 4\\ 5
			\end{bmatrix} \right\rbrace } 
	$$
	
	\subsubsection*{(b)} \textbf{The line in the direction $\begin{bmatrix}
		2 \\ 1 \\ 3
		\end{bmatrix}$} 
		
		Need $\begin{bmatrix}
		a \\ b \\ c
		\end{bmatrix} \cdot \begin{bmatrix}
		2 \\ 1 \\ 3
		\end{bmatrix}$ 
		
		Arbitrarily choose $\begin{bmatrix}
		1 \\ 1\\ -1
		\end{bmatrix}$
		
		Now we need another basis vector that's perpendicular to the other two-- we can use our old friend cross product. 
		
		$$
		\begin{bmatrix}
		2 \\ 1\\ 3
		\end{bmatrix} \times \begin{bmatrix}
		1 \\ 1\\ -1
		\end{bmatrix} = \begin{bmatrix}
		-4 \\ 1\\ 3
		\end{bmatrix}
		$$ 
		
		And so a basis is the following
		$$
		\boxed{\text{span} \left\lbrace \begin{bmatrix}
		1 \\ 1 \\ -1
		\end{bmatrix}, \begin{bmatrix}
		-4 \\ 3 \\ 1
		\end{bmatrix} \right\rbrace}
		$$
	\subsubsection*{(c)} \textbf{Range of the matrix $A = \begin{bmatrix}
		1 & 2 & -1 & 3 \\ -2 & 0 & 2 & 1 \\ 1 & 2 & 1 & 4
		\end{bmatrix}$}
		First we need to actually \textit{find} the range.
		\begin{align*}
		& A = \begin{bmatrix}
		1 & 2 & -1 & 3 \\ -2 & 0 & 2 & 1 \\ 1 & 2 & 1 & 4
		\end{bmatrix} \begin{array}{c}
		\\ + 2 R_1 \\ + R_1
		\end{array} = \begin{bmatrix}
		1 & 2 & -1 & 3 \\ 0 & 4 & 0 & 7 \\ 0 & 4 & 0 & 7
		\end{bmatrix} \begin{array}{c}
		\\ \\ -R_2
		\end{array} \\ 
		& = \begin{bmatrix}
		1 & 2 & -1 & 3 \\ 0 & 4 & 0 & 7 \\ 0 & 0 & 0 & 0
		\end{bmatrix}
		\end{align*}
		Now we know that the columns where a pivot point appears in the reduced form (the first 2) can be taken from the original matrix and be used as the range of the matrix. 
		
		We need something orthogonal to both of those, so we'll use the cross product again because we don't like change. 
		$$
		\begin{bmatrix}
		1 \\-2 \\ -1
		\end{bmatrix} \times \begin{bmatrix}
		2 \\ 0 \\ 2
		\end{bmatrix} = \begin{bmatrix}
		-4 \\ -4 \\ 4
		\end{bmatrix}
		$$
		
		So the basis is $\boxed{\text{span} \left\lbrace \begin{bmatrix}
			-1 \\ -1 \\ 1
			\end{bmatrix} \right\rbrace }$
	\subsubsection*{(d)} \textbf{The cokernel of $A$ from the previous part}
	\begin{align*}
	& A^T = \begin{bmatrix}
	1 & -2 & 1\\ 2 & 0 & 2\\ -1 & 2 & 1\\ 3 & 1 & 4 
	\end{bmatrix} \begin{array}{c}
	 \\ -2 R_1 \\ +R_1 \\ -3R_1
	\end{array} = \begin{bmatrix}
	1 & -2 & 1\\ 0 & 4 & 0\\ 0 & 0 & 2\\ 0 & 7 & 1 
	\end{bmatrix} \begin{array}{c}
	\\ \\ \\ -\frac{7}{4} R_2
	\end{array} = \begin{bmatrix}
	1 & -2 & 1\\ 0 & 4 & 0\\ 0 & 0 & 2\\ 0 & 0 & 1 
	\end{bmatrix}  \begin{array}{c}
	\\ \\ \\ -\frac{1}{2}R_3
	\end{array} \\ 
	& = \begin{bmatrix}
	1 & -2 & 1\\ 0 & 4 & 0\\ 0 & 0 & 2\\ 0 & 0 & 0 
	\end{bmatrix} \begin{bmatrix}
	a \\ b \\ c
	\end{bmatrix} = \begin{bmatrix}
	0 \\ 0 \\ 0 \\ 0
	\end{bmatrix} \begin{array}{c}
	a -2b + c = 0 \\ 4b = 0 \\ c = 0
	\end{array}
	\end{align*} 
	Obviously $a, b$ and $c$ are all 0, so the only element in the cokernel of $A$ is the zero vector. All vectors are orthogonal to the zero vector, so in a sense all of $\R^3$ is orthogonal to the cokernel. We can use the trivial basis $\hat{i}, \hat{j}, \hat{k}$. 
	
	\subsection*{8}
	\begin{proof}
		\textbf{Seek to prove that the orthogonal complement $W^\perp$ to a subspace is itself a subspace.}
		
		We're working with a subspace of an existing, valid subspace, so we only need to demonstrate that our subspace is closed under addition, scalar multiplication, and contains the zero vector. 
		
		Showing that the subspace contains the zero vector is trivial, since by definition $W \cap W^\perp = \{0\}$
		
		Also, for any $u,v \: \epsilon W^\perp$ which satisfy $\langle u, w\rangle = 0, \langle v, w\rangle = 0 $ for any $w \: \epsilon W$, $\langle c_1u + c_2v,w \rangle = c_1 \langle u,w \rangle + c_2 \langle v,w \rangle $, which shows that $c_1u + c_2v \: \epsilon \: W^\perp$.  This demonstrates that the subspace is closed under both addition and scalar multiplication. 
	\end{proof}
	\subsection*{15} Let $W \subset V$ with $\dim V = n$.  Suppose $w_1, \cdots, w_m$ is an orthogonal basis for $W$, and $w_{m+1},\cdots,w_n$ is an orthogonal basis for $W^\perp$.
	\subsubsection*{(a)} \begin{proof}
		\textbf{Seek to prove that $w_1,\cdots,w_n$ forms an orthogonal basis for $V$.}
		
		We know that both $W$ and $W^\perp$ are composed of vectors that are all orthogonal to each other, since they form an orthogonal basis for each of their respective subspaces. We also know that every element in $W^\perp$ is orthogonal to all vectors in $W$, by definition. The converse, (All $W$ are orthogonal to all $W^\perp$) must be true by \textit{Proposition 5.53}, $(W^\perp)^\perp = W.$
		
		Therefore we have exactly $n$ vectors all orthogonal to each other, which \textit{must} form a bbasis for $V$, since $\dim V = n$. 
	\end{proof}
	\subsubsection*{(b)} \textbf{Show that if $v = c_1w_1 + \cdots + c_nw_n$ is any vector in $V$, then its orthogonal decomposition is equal to $v = w+z$, where $w = c_1w_1 + \cdots + c_mw_m \: \epsilon \: W$, and $z = c_{m+1}w_{m+1}+\cdots + c_nw_n \: \epsilon \: W^\perp$}
	
	The possible values for $w$ span all of $W$, and the options for $z$ span all of $W^\perp$. \textit{Proposition 5.49} states that every vector $v$ can be uniquely decomposed into $v = w + z$, $w \: \epsilon \: W$, $z \: \epsilon \: W^\perp$, which obviously shows what we seek to demonstrate. 
	 
	\subsection*{17} \textbf{For the following matrix, $(i)$ find a basis for the 4 fundamental subspaces, $(ii)$ verify that the range and cokernel are orthogonal complements, and $(iii)$ verify that the corange and kernel are orthogonal complements. }
	\subsubsection*{(c)} \textbf{$A = \begin{bmatrix}
		0 & 1 & 2 \\ -1 & 0 & -3 \\ -2 & 3 & 0
		\end{bmatrix}$} 
	\begin{align*}
	& A = \begin{bmatrix}
	0 & 1 & 2 \\ -1 & 0 & -3 \\ -2 & 3 & 0
	\end{bmatrix} \begin{array}{c}
	R_1 = R_2 \\ R_2 = R_1 \\ .
	\end{array} = \begin{bmatrix}
	-1 & 0 & -3 \\ 0 & 1 & 2 \\  -2 & 3 & 0
	\end{bmatrix} \begin{array}{c}
	\\ \\ -2R_1 
	\end{array} = \begin{bmatrix}
	-1 & 0 & -3 \\ 0 & 1 & 2 \\  0 & 3 & 6
	\end{bmatrix} \begin{array}{c}
	R_1 = (-1)R_1\\ \\ -3R_2
	\end{array} \\ 
	& = \begin{bmatrix}
	-1 & 0 & -3 \\ 0 & 1 & 2 \\  0 & 0 & 0
	\end{bmatrix}
	\end{align*}
	We found the reduced row echelon form of the matrix, with pivots in the first two columns.  We can use the first two columns of $A$ as the basis of the range of $A$. 
	
	$$
	\boxed{\text{range}(A) = \text{span} \left\lbrace \begin{bmatrix}
	0 \\ -1 \\ -2
	\end{bmatrix}, \begin{bmatrix}
	1 \\ 0 \\ 3
	\end{bmatrix} \right\rbrace }
	$$
	
	Now we can just find the kernel of $A$: 
	$$
	\begin{bmatrix}
	-1 & 0 & -3 \\ 0 & 1 & 2 \\  0 & 0 & 0
	\end{bmatrix} \begin{bmatrix}
	a \\ b \\ c
	\end{bmatrix} = \begin{bmatrix}
	0 \\ 0 \\0 
	\end{bmatrix}, \begin{array}{c} a + 3c = 0 \\ b+2c = 0
	\end{array}
	$$
	
	$$
	\boxed{\ker(A) = \text{span} \left\lbrace \begin{bmatrix}
	3 \\ 2 \\ -1
	\end{bmatrix} \right\rbrace}
	$$
	
	Finding the corange and cokernel are a bit easier now, especially since $A$ is skew-symmetric.  This means that $A$ and $A^T$ have the \textit{same} reduced row echelon form, and that we can just take the first two columns of $A^T$ to be the basis of the corange. 
	
	
	$$
	\boxed{\text{corange}(A) = \text{span} \left\lbrace \begin{bmatrix}
		0 \\ 1 \\ 2
	\end{bmatrix}, \begin{bmatrix}
	-1 \\ 0 \\ -3
	\end{bmatrix} \right\rbrace}
	$$
	
	Since the reduced row echelon form is the same, the cokernel is the same as the kernel. 
	
	$$
	\boxed{\text{coker} (A) = \ker(A) = \text{span} \left\lbrace \begin{bmatrix}
	3 \\ 2 \\ -1
	\end{bmatrix} \right\rbrace}
	$$
	
	Now we verify that the range and cokernel are orthogonal complements. We'll just work with the bases chosen above instead of writing ``span'' all the time, but the method holds for all multiples of the vectors as well. 
	
	\begin{align*}
	& \ker(A) = \begin{bmatrix}
	3 \\ 2 \\ -1
	\end{bmatrix} & \text{range}(A) = \begin{bmatrix}
	0 \\ -1 \\ -2
	\end{bmatrix}, \begin{bmatrix}
	1 \\ 0 \\ 3
	\end{bmatrix} \\
	& \boxed{\begin{bmatrix}
	3 \\ 2 \\ -1
	\end{bmatrix} \cdot \begin{bmatrix}
	0 \\ -1 \\ -2
	\end{bmatrix} = 0} & \boxed{\begin{bmatrix}
	3 \\ 2 \\ -1
	\end{bmatrix} \cdot \begin{bmatrix}
	1 \\ 0 \\ 3
	\end{bmatrix} = 0}
	\end{align*}
	
	So indeed, the range and cokernel are orthogonal complements. 
	
	We also need to show that the corange and the kernel are orthogonal complements. 
	
	\begin{align*}
	& \text{co}\ker(A) = \begin{bmatrix}
	3 \\ 2 \\ -1
	\end{bmatrix} & \text{corange}(A) = \begin{bmatrix}
	0 \\ 1 \\ 2
	\end{bmatrix}, \begin{bmatrix}
	-1 \\ 0 \\ -3
	\end{bmatrix} \\
	& \boxed{\begin{bmatrix}
		3 \\ 2 \\ -1
		\end{bmatrix} \cdot \begin{bmatrix}
		0 \\ 1 \\ 2
		\end{bmatrix} = 0} & \boxed{\begin{bmatrix}
		3 \\ 2 \\ -1
		\end{bmatrix} \cdot \begin{bmatrix}
		-1 \\ 0 \\ -3
		\end{bmatrix} = 0}
	\end{align*}
	
	Which confirms that the corange and kernel are orthogonal complements. 
		
	\subsection*{22} \textbf{For each of the following linear systems, verify compatibility using ther Fredholm alternative, find the general solution, and find the solution of minimum Euclidean norm}
	\subsubsection*{(a)} 
	$$
	\begin{array}{c}
	2x - 4y = -6 \\ -x + 2y = 3
	\end{array}
	$$
	Or 
	$$
	\begin{bmatrix}
	2 & -4 \\ -1 & 2
	\end{bmatrix} \begin{bmatrix}
	x \\ y
	\end{bmatrix} = \begin{bmatrix}
	-6 \\ 3
	\end{bmatrix}
	$$
	We know that the system has a solution only if $b$ is orthogonal to the corkernel of $A$. 
	
	The cokernel of $A$ is the span of $\begin{bmatrix}
	1 \\ 2
	\end{bmatrix}$, and $\begin{bmatrix}
	1 \\ 2 
	\end{bmatrix} \cdot \begin{bmatrix}
	\-6 \\ 3
	\end{bmatrix} = 0$, \textbf{so the system has a solution! }
	
	$$
	\begin{bmatrix}
	2 & -4 \\ -1 & 2
	\end{bmatrix} \begin{array}{c}
	\\ +\frac{R_1}{2}
	\end{array} = \begin{bmatrix}
	2 & -4 \\ 0 & 0 
	\end{bmatrix} \begin{bmatrix}
	x \\ y
	\end{bmatrix} = \begin{bmatrix}
	-6 \\ 3
	\end{bmatrix}, \begin{array}{c}
	2x - 4y = -6
	\end{array}
	$$
	
	And so the system has an infinite number of solutions, \textbf{with the general solution} $x = 2y -3$, with $y$ being a free variable. 
	
	Now we need to find the minimum Euclidean norm, or the minimum of $\sqrt{x^2 + y^2}$. This is just a calculus problem. 
	$$
	\sqrt{x^2 + y^2} = \sqrt{(2y -3 )^2 + y^2} = \sqrt{5y^2 -12y + 9}
	$$
	Take the derivate, set it equal to 0
	$$
	\frac{10y - 12}{\sqrt{5y^2 -12y + 9}} = 0 \Rightarrow 10y - 12 = 0, \: y = \frac{6}{5}
	$$
	
	From this it's easy to see that $x = \frac{-3}{5}$. And so \textbf{the minimum is } $\boxed{(x,y) = (-\frac{3}{5}, \frac{6}{5})}$
	\subsubsection*{(f)}
	$$
	A = \begin{bmatrix}
	1 & -1 & 2 & 3 \\ 3 & -3 & 5 & 7 \\ -2 & 2 & 1 & 4
	\end{bmatrix} \qquad b = \begin{bmatrix}
	5 \\ 13 \\ 0
	\end{bmatrix}
	$$
\end{document}